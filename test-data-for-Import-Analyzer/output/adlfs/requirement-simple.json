{
  "name": "adlfs",
  "description": "Filesystem interface to Azure-Datalake Gen1 and Gen2 Storage \n------------------------------------------------------------\n\n\n[![PyPI version shields.io](https://img.shields.io/pypi/v/adlfs.svg)](https://pypi.python.org/pypi/adlfs/)\n[![Latest conda-forge version](https://img.shields.io/conda/vn/conda-forge/adlfs?logo=conda-forge)](https://anaconda.org/conda-forge/aldfs)\n\nQuickstart\n----------\n\nThis package can be installed using:\n\n`pip install adlfs`\n\nor\n\n`conda install -c conda-forge adlfs`\n\nThe `adl://` and `abfs://` protocols are included in fsspec's known_implementations registry \nin fsspec > 0.6.1, otherwise users must explicitly inform fsspec about the supported adlfs protocols.\n\n\nTo use the Gen1 filesystem:\n\n```python\nimport dask.dataframe as dd\n\nstorage_options={'tenant_id': TENANT_ID, 'client_id': CLIENT_ID, 'client_secret': CLIENT_SECRET}\n\ndd.read_csv('adl://{STORE_NAME}/{FOLDER}/*.csv', storage_options=storage_options)\n```\n\nTo use the Gen2 filesystem you can use the protocol `abfs` or `az`:\n\n```python\nimport dask.dataframe as dd\n\nstorage_options={'account_name': ACCOUNT_NAME, 'account_key': ACCOUNT_KEY}\n\nddf = dd.read_csv('abfs://{CONTAINER}/{FOLDER}/*.csv', storage_options=storage_options)\nddf = dd.read_parquet('az://{CONTAINER}/folder.parquet', storage_options=storage_options)\n\nor optionally, if AZURE_STORAGE_ACCOUNT_NAME and an AZURE_STORAGE_<CREDENTIAL> is \nset as an environmental variable, then storage_options will be read from the environmental\nvariables\n```\n\nTo read from a public storage blob you are required to specify the `'account_name'`.\nFor example, you can access [NYC Taxi & Limousine Commission](https://azure.microsoft.com/en-us/services/open-datasets/catalog/nyc-taxi-limousine-commission-green-taxi-trip-records/) as:\n\n```python\nstorage_options = {'account_name': 'azureopendatastorage'}\nddf = dd.read_parquet('az://nyctlc/green/puYear=2019/puMonth=*/*.parquet', storage_options=storage_options)\n```\n\n\nDetails\n-------\nThe package includes pythonic filesystem implementations for both \nAzure Datalake Gen1 and Azure Datalake Gen2, that facilitate \ninteractions between both Azure Datalake implementations and Dask.  This is done leveraging the \n[intake/filesystem_spec](https://github.com/intake/filesystem_spec/tree/master/fsspec) base class and Azure Python SDKs.\n\nOperations against both Gen1 Datalake currently only work with an Azure ServicePrincipal\nwith suitable credentials to perform operations on the resources of choice.\n\nOperations against the Gen2 Datalake are implemented by leveraging [Azure Blob Storage Python SDK](https://github.com/Azure/azure-sdk-for-python).\n\n    The filesystem can be instantiated with a variety of credentials, including:\n        account_name\n        account_key\n        sas_token\n        connection_string\n        Azure ServicePrincipal credentials (which requires tenant_id, client_id, client_secret)\n        location_mode:  valid value are \"primary\" or \"secondary\" and apply to RA-GRS accounts\n\n    The following enviornmental variables can also be set and picked up for authentication:\n        \"AZURE_STORAGE_CONNECTION_STRING\"\n        \"AZURE_STORAGE_ACCOUNT_NAME\"\n        \"AZURE_STORAGE_ACCOUNT_KEY\"\n        \"AZURE_STORAGE_SAS_TOKEN\"\n        \"AZURE_STORAGE_CLIENT_SECRET\"\n        \"AZURE_STORAGE_CLIENT_ID\"\n        \"AZURE_STORAGE_TENANT_ID\"\n\n\nThe AzureBlobFileSystem accepts [all of the Async BlobServiceClient arguments](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python).\n\n    By default, write operations create BlockBlobs in Azure, which, once written can not be appended.  It is possible to create an AppendBlob using an `mode=\"ab\"` when creating, and then when operating on blobs.  Currently AppendBlobs are not available if hierarchical namespaces are enabled.",
  "requires_dist": null,
  "requires_python": ">3.6",
  "current_version": "0.7.5",
  "released_versions": [
    "0.0.10",
    "0.0.10.post0",
    "0.0.10.post1",
    "0.0.10.post2",
    "0.0.11",
    "0.0.2",
    "0.0.5",
    "0.0.5a0",
    "0.0.6",
    "0.0.7",
    "0.0.8",
    "0.0.8.post0",
    "0.0.8.post1",
    "0.0.8.post2",
    "0.0.8.post3",
    "0.0.8a0",
    "0.0.9",
    "0.0.9.post0",
    "0.1.0",
    "0.1.1",
    "0.1.2",
    "0.1.3",
    "0.1.3a0",
    "0.1.4",
    "0.1.5",
    "0.2.0",
    "0.2.2",
    "0.2.3",
    "0.2.4",
    "0.2.5",
    "0.3.0",
    "0.3.1",
    "0.3.2",
    "0.3.3",
    "0.4.0",
    "0.5.0",
    "0.5.1",
    "0.5.2",
    "0.5.3",
    "0.5.4",
    "0.5.5",
    "0.5.7",
    "0.5.8",
    "0.5.9",
    "0.6.0",
    "0.6.1",
    "0.6.2",
    "0.6.3",
    "0.7.0",
    "0.7.1",
    "0.7.2",
    "0.7.3",
    "0.7.4",
    "0.7.5"
  ]
}